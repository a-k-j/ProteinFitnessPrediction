import os
import glob
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
from scipy.stats import spearmanr
from tqdm import tqdm
import matplotlib.pyplot as plt

# Constants
SEED = 42
BATCH_SIZE = 32
LEARNING_RATE = 5e-5
WEIGHT_DECAY = 1e-6
NUM_EPOCHS = 50
PATIENCE = 5
EMBEDDINGS_DIR = "/scratch/akj/DMS_ProteinGym_embeddings"  # Directory with sequence embeddings
STRUCT_EMBEDDINGS_FILE = "/scratch/akj/structural_embeddings.npy"
OUTPUT_DIR = "/scratch/akj/model_results"
SAMPLES_PER_FILE = 10
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

class CombinedModel(nn.Module):
    """
    MLP for late fusion of sequence and structure embeddings,
    as described in the paper.
    """
    def __init__(self, seq_dim, struct_dim, hidden_dim=512):
        super(CombinedModel, self).__init__()
        
        combined_dim = seq_dim + struct_dim
        
        self.layer1 = nn.Linear(combined_dim, hidden_dim)
        self.dropout = nn.Dropout(0.2)
        self.layer2 = nn.Linear(hidden_dim, 1)
        
    def forward(self, x):
        x = self.layer1(x)
        x = torch.relu(x)
        x = self.dropout(x)
        x = self.layer2(x)
        return x.squeeze()

def load_embedding_data():
    """Load embedding data from all files and take the first SAMPLES_PER_FILE samples from each."""
    all_embedding_files = glob.glob(os.path.join(EMBEDDINGS_DIR, "*_with_embeddings.csv"))
    
    # Check if we found any files
    if not all_embedding_files:
        raise ValueError(f"No embedding files found in {EMBEDDINGS_DIR}")
    
    print(f"Found {len(all_embedding_files)} embedding files")
    
    all_data_samples = []
    
    for file_path in tqdm(all_embedding_files, desc="Loading data"):
        try:
            # Load the file
            df = pd.read_csv(file_path)
            
            # Check if the file has the required columns
            embedding_cols = [col for col in df.columns if 'embedding_' in col]
            if not embedding_cols or 'DMS_score' not in df.columns:
                print(f"Skipping {file_path}: Missing required columns")
                continue
            
            # Take first N samples
            samples = df.head(SAMPLES_PER_FILE)
            if len(samples) > 0:
                all_data_samples.append(samples)
        except Exception as e:
            print(f"Error processing {file_path}: {str(e)}")
    
    # Combine all samples
    if not all_data_samples:
        raise ValueError("No valid data found in any of the embedding files")
    
    combined_data = pd.concat(all_data_samples, ignore_index=True)
    print(f"Total samples collected: {len(combined_data)}")
    
    return combined_data

def load_structural_embeddings():
    """Load structural embeddings generated by the CNN."""
    if not os.path.exists(STRUCT_EMBEDDINGS_FILE):
        raise ValueError(f"Structural embeddings file not found: {STRUCT_EMBEDDINGS_FILE}")
    
    structural_embeddings = np.load(STRUCT_EMBEDDINGS_FILE)
    print(f"Loaded {len(structural_embeddings)} structural embeddings")
    
    return structural_embeddings

def combine_embeddings(sequence_data, structural_embeddings):
    """Combine sequence and structural embeddings based on the order they appear in the files."""
    # Extract sequence embeddings
    embedding_cols = [col for col in sequence_data.columns if 'embedding_' in col]
    sequence_embeddings = sequence_data[embedding_cols].values
    
    num_seq = len(sequence_embeddings)
    num_struct = len(structural_embeddings)
    
    print(f"Number of sequence embeddings: {num_seq}")
    print(f"Number of structural embeddings: {num_struct}")
    
    # Use the minimum number to avoid index errors
    num_to_use = min(num_seq, num_struct)
    print(f"Using the first {num_to_use} embeddings for training")
    
    # Combine embeddings
    combined_embeddings = []
    for i in range(num_to_use):
        seq_emb = sequence_embeddings[i]
        struct_emb = structural_embeddings[i]
        
        # Concatenate embeddings
        combined_emb = np.concatenate([seq_emb, struct_emb])
        combined_embeddings.append(combined_emb)
    
    # Convert to numpy array
    combined_embeddings = np.array(combined_embeddings)
    
    # Get corresponding DMS scores
    dms_scores = sequence_data.iloc[:num_to_use]['DMS_score'].values
    
    print(f"Successfully combined {len(combined_embeddings)} embeddings")
    
    return combined_embeddings, dms_scores

def prepare_data(X, y):
    """Prepare data for training and testing."""
    # Split into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)
    
    # Convert to tensors
    X_train_tensor = torch.FloatTensor(X_train)
    y_train_tensor = torch.FloatTensor(y_train)
    X_test_tensor = torch.FloatTensor(X_test)
    y_test_tensor = torch.FloatTensor(y_test)
    
    # Create datasets and dataloaders
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)
    
    return train_loader, test_loader, X_test_tensor, y_test_tensor

def train_model(model, train_loader, test_loader, X_test, y_test):
    """Train the model and evaluate on test data using Spearman correlation."""
    model = model.to(device)
    criterion = nn.MSELoss()  
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    
    best_spearman = -1.0
    best_model_state = None
    patience_counter = 0
    training_losses = []
    test_spearman_scores = []
    
    for epoch in range(NUM_EPOCHS):
        # Training
        model.train()
        train_loss = 0.0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            
            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item() * inputs.size(0)
        
        train_loss = train_loss / len(train_loader.dataset)
        training_losses.append(train_loss)
        
        # Evaluation
        model.eval()
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for inputs, targets in test_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                all_preds.extend(outputs.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())
            
        # Calculate Spearman correlation
        spearman_corr, _ = spearmanr(all_targets, all_preds)
        test_spearman_scores.append(spearman_corr)
        
        print(f"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, "
              f"Test Spearman Correlation: {spearman_corr:.4f}")
        
        # Check if this is the best model so far
        if spearman_corr > best_spearman:
            best_spearman = spearman_corr
            best_model_state = model.state_dict().copy()
            patience_counter = 0
        else:
            patience_counter += 1
        
        # Early stopping
        if patience_counter >= PATIENCE:
            print(f"Early stopping after {epoch+1} epochs")
            break
    
    # Load the best model
    model.load_state_dict(best_model_state)
    
    # Final evaluation on the test set
    model.eval()
    with torch.no_grad():
        y_pred = model(X_test.to(device)).cpu().numpy()
    
    # Calculate final metric
    final_spearman, p_value = spearmanr(y_test.numpy(), y_pred)
    
    print(f"\nFinal Results:")
    print(f"Test Spearman Correlation: {final_spearman:.4f}")
    
    return final_spearman

def main():
    try:
        # 1. Load sequence embedding data
        sequence_data = load_embedding_data()
        
        # 2. Load structural embeddings
        structural_embeddings = load_structural_embeddings()
        
        # 3. Combine embeddings based on order
        combined_embeddings, dms_scores = combine_embeddings(sequence_data, structural_embeddings)
        
        # 4. Prepare data for training
        train_loader, test_loader, X_test, y_test = prepare_data(combined_embeddings, dms_scores)
        
        # 5. Initialize and train the model
        seq_dim = len([col for col in sequence_data.columns if 'embedding_' in col])
        struct_dim = structural_embeddings.shape[1]
        print(f"Sequence embedding dimension: {seq_dim}")
        print(f"Structural embedding dimension: {struct_dim}")
        
        model = CombinedModel(seq_dim=seq_dim, struct_dim=struct_dim)
        spearman_corr = train_model(model, train_loader, test_loader, X_test, y_test)
        
        print("\nTraining completed successfully!")
        print(f"Final Spearman Correlation: {spearman_corr:.4f}")
        
    except Exception as e:
        print(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    main()